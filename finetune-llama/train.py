# referenced from https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/
# with a few changes/minimizations

def train(model, tokenizer, dataset, outdir):
     # enable gradient checkpointing to reduce memory usage during fine-tuning


    pass
    

def main():
    pass

if __name__ == "__main__":
    main()